{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData\n",
    "import json\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy.types import VARCHAR\n",
    "from pyodbc import connect, SQL_WVARCHAR\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(name):\n",
    "    \"\"\"Generate a logger with a given name.\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    file_formatter = logging.Formatter(\n",
    "        '%(levelname)s - %(asctime)s - %(message)s - %(module)s',\n",
    "        \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    file_handler = logging.FileHandler(\"logfile.log\")\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # console_formatter = logging.Formatter(\n",
    "    #     '%(levelname)s - %(asctime)s - %(message)s - %(module)s',\n",
    "    #     \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # console_handler = logging.StreamHandler()\n",
    "    # console_handler.setLevel(logging.INFO)\n",
    "    # console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#     'destination': {\n",
    "#         'drivername': 'postgresql+psycopg2',\n",
    "#         'username': '',\n",
    "#         'password': '',\n",
    "#         'host': '',\n",
    "#         'port': '',\n",
    "#         'database': ''\n",
    "#     },\n",
    "#     'origin': {\n",
    "#         'drivername': 'postgresql+psycopg2',\n",
    "#         'username': '',\n",
    "#         'password': '',\n",
    "#         'host': '',\n",
    "#         'port': '',\n",
    "#         'database': ''\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "with open('credentials.json', 'r') as file:\n",
    "    CREDS = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_ORIGIN = URL.create(**CREDS['origin'])\n",
    "URL_DESTINATION = URL.create(**CREDS['destination'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGIN_ENGINE = create_engine(URL_ORIGIN, connect_args={'sslmode': 'prefer'})\n",
    "DESTINATION_ENGINE = create_engine(URL_DESTINATION, connect_args={'sslmode': 'prefer'})\n",
    "ODBC_CREDS = {\n",
    "    'Driver': '/opt/amazon/redshift/lib/libamazonredshiftodbc.dylib',\n",
    "    'Server': CREDS['destination']['host'],\n",
    "    'Database': CREDS['destination']['database'],\n",
    "    'UID': CREDS['destination']['username'],\n",
    "    'PWD': CREDS['destination']['password'],\n",
    "    'Port': CREDS['destination']['port'],\n",
    "    'BoolsAsChar': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicateSchema():\n",
    "    def __init__(self, origin_engine, destination_engine):\n",
    "        # SQLalchemy engines for origin and destination DB.\n",
    "        self.origin_engine = origin_engine\n",
    "        self.destination_engine = destination_engine\n",
    "        # Meta data objects.\n",
    "        self.origin_engine_meta = MetaData(bind=origin_engine)\n",
    "        self.destination_engine_meta = MetaData(bind=destination_engine)\n",
    "        # Schema tracker variable for _meta_refresch method.\n",
    "        self.previous_schema = ''\n",
    "\n",
    "    def _meta_refresh(self, schema_name):\n",
    "        \"\"\"Refresh origin db metadata, when necessary.\n",
    "        \n",
    "        The relfect() method takes a while to run. This function makes sure\n",
    "        that it only runs when users switch to new Redshift schema.\n",
    "        \"\"\"\n",
    "        if self.previous_schema != schema_name:\n",
    "            LOGGER.info(\"Set sqlalchemy meta schema to %s.\", schema_name)\n",
    "            self.origin_engine_meta.clear()\n",
    "            self.origin_engine_meta.reflect(schema=schema_name)\n",
    "            self.previous_schema = schema_name\n",
    "        return None\n",
    "    \n",
    "    def setup_schema(self, schema_name):\n",
    "        \"\"\"Create schema only in destination db.\"\"\"\n",
    "        LOGGER.info(\"Creating the %s schema.\", schema_name)\n",
    "        with self.destination_engine.connect() as con:\n",
    "            con.execute('CREATE SCHEMA IF NOT EXISTS {};'.format(schema_name))\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def _remove_identity_clause(self, table_name):\n",
    "        \"\"\"Removes indenity and default column values.\n",
    "        \n",
    "        Trying to insert values into an identity/serial column causes errors.\n",
    "        As a work around, it is possible to remove these restrictions from\n",
    "        the metadata file, so that they do not get implemented in destination\n",
    "        database.\n",
    "        \"\"\"\n",
    "        table = self.origin_engine_meta.tables[table_name]\n",
    "\n",
    "        for column in table.columns:\n",
    "            column.server_default = None\n",
    "\n",
    "        return None\n",
    "\n",
    "    def create_1_table(self, schema_name, table_name):\n",
    "        \"\"\"Create one empty table in destination db.\"\"\"\n",
    "        LOGGER.info(\"Creating the %s table.\", table_name)\n",
    "        self._meta_refresh(schema_name)\n",
    "        \n",
    "        self._remove_identity_clause(table_name)\n",
    "\n",
    "        self.origin_engine_meta.tables[table_name].create(bind=self.destination_engine)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def create_all_tables(self, schema_name):\n",
    "        \"\"\"Creates all tables in a given schema in desitnation db.\"\"\"\n",
    "        LOGGER.info(\"Creating all tables in %s schema.\", schema_name)\n",
    "\n",
    "        self._meta_refresh(schema_name)\n",
    "\n",
    "        tables = self.origin_engine_meta.tables\n",
    "\n",
    "        for table in tables:\n",
    "            self._remove_identity_clause(table)\n",
    "\n",
    "        self.origin_engine_meta.create_all(bind=self.destination_engine)\n",
    "\n",
    "    def create_full_schema(self, schema_name):\n",
    "        \"\"\"Creates both schema and corresponding table in destination db.\"\"\"\n",
    "        self._meta_refresh(schema_name)\n",
    "\n",
    "        self.setup_schema(schema_name)\n",
    "\n",
    "        self.create_all_tables(schema_name)\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleData(DuplicateSchema):\n",
    "    \"\"\"Populate newly duplicated tables in new db with sample data from origin db.\"\"\"\n",
    "    def __init__(self, origin_engine, destination_engine, odbc_creds):\n",
    "        self.origin_engine = origin_engine\n",
    "        self.destination_engine = destination_engine\n",
    "        self.origin_engine_meta = MetaData(bind=origin_engine)\n",
    "        self.destination_engine_meta = MetaData(bind=destination_engine)\n",
    "        self.odbc_creds = odbc_creds\n",
    "        self.odbc_connection = connect(**self.odbc_creds)\n",
    "        self.previous_schema = ''\n",
    "\n",
    "    def _get_data_sample(self, table_name, sample_size):\n",
    "        \"\"\"Fetch a sample of rows from a single table in origin db.\"\"\"\n",
    "        LOGGER.info(\"Fetch data sample from %s table.\", table_name)\n",
    "\n",
    "        table =  self.origin_engine_meta.tables[table_name]\n",
    "\n",
    "        cursor = table.select().limit(sample_size).execute()\n",
    "\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        return rows\n",
    "\n",
    "    def _odbc_executemany_args(self, table_name, rows):\n",
    "        \"\"\"Generate inputs for the ODBC insert query.\n",
    "        The query needs to have as many question marks as variables in the data sample.\n",
    "        Each row of variables must be a tuple in a list.\n",
    "        \"\"\"\n",
    "        LOGGER.info(\"Generate odbc arguments for the %s table.\", table_name)\n",
    "\n",
    "        query_template = \"INSERT INTO {table_name} VALUES ({question_marks})\"\n",
    "\n",
    "        # Create as many quetion marks as variables in source data.\n",
    "        question_mark_list = ['?'] * len(rows[0])\n",
    "\n",
    "        # Generate the query with table name and question marks.\n",
    "        insert_query = query_template.format(\n",
    "            table_name = table_name,\n",
    "            question_marks = \", \".join(question_mark_list)\n",
    "        )\n",
    "\n",
    "        list_of_tuples = [tuple(x) for x in rows]\n",
    "\n",
    "        return insert_query, list_of_tuples\n",
    "        \n",
    "    def _odbc_data_types(self, table_name):\n",
    "        \"\"\"Overwrite pyodbc character limit on VARCHAR.\n",
    "        \n",
    "        There is a well known issue with pyodbc module, which happens\n",
    "        when trying to insert long strinds into large TEXT or VARCHAR\n",
    "        columns. Even if the destination column is big enough, the module\n",
    "        will throw out an overflow error. Manually overriding the data types\n",
    "        used in the cursor is one workaround. This function generates a list of \n",
    "        tuples, which modify the data types of large VARCHAR columns.\n",
    "        \"\"\"\n",
    "        LOGGER.info(\"Generate data types for the %s table.\", table_name)\n",
    "\n",
    "        columns = self.origin_engine_meta.tables[table_name].columns\n",
    "\n",
    "        # Generate a list of None values. If there are no large VARCHAR columns\n",
    "        # the cursor will use its defaults. \n",
    "        result = [None] * len(columns)\n",
    "\n",
    "        # If a given column is of type VARCHAR and has more than 100 char length,\n",
    "        # None value in the list is replaced with a data type tuple.\n",
    "        for i, column in enumerate(columns):            \n",
    "            if type(column.type) == VARCHAR:\n",
    "                if column.type.length >= 100:\n",
    "                    result[i] = (SQL_WVARCHAR, 100000, 0)\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "    def populate_1_table(self, schema_name, table_name, sample_size):\n",
    "        \"\"\"Fetch a data sample and insert it into a single destiontion table.\"\"\"\n",
    "        LOGGER.info(\"Being populating the %s table.\", table_name)\n",
    "\n",
    "        self._meta_refresh(schema_name)\n",
    "        \n",
    "        rows = self._get_data_sample(table_name, sample_size)\n",
    "\n",
    "        if len(rows) == 0:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "\n",
    "            insert_query, list_of_tuples = self._odbc_executemany_args(table_name, rows)\n",
    "\n",
    "            data_types = self._odbc_data_types(table_name)\n",
    "            \n",
    "            cursor = self.odbc_connection.cursor()\n",
    "            # Here, the list of typles is used to overwrite certain column data types.\n",
    "            cursor.setinputsizes(data_types)\n",
    "            cursor.fast_executemany = True\n",
    "\n",
    "            cursor.executemany(insert_query, list_of_tuples)\n",
    "\n",
    "            cursor.commit()\n",
    "            cursor.close()\n",
    "            \n",
    "            return None\n",
    "    \n",
    "    def populate_all_tables(self, schema_name, sample_size):\n",
    "        \"\"\"Populate all tables in a given destination schema with sample data.\"\"\"\n",
    "        LOGGER.info(\"Begin populating tables in the %s schema.\", schema_name)\n",
    "\n",
    "        self._meta_refresh(schema_name)\n",
    "\n",
    "        table_names = list(self.origin_engine_meta.tables.keys())\n",
    "\n",
    "        for table in table_names:\n",
    "            self.populate_1_table(schema_name, table, sample_size)\n",
    "        \n",
    "        return None\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anon_users",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
